# ğŸ” DocRag - Agentic RAG Document Search System

A powerful Retrieval-Augmented Generation (RAG) system built with LangChain, LangGraph, and Streamlit. DocRag enables intelligent document search and question-answering by combining document retrieval with large language models.

## âœ¨ Features

- **ğŸŒ Multi-Source Document Loading**: Support for web URLs, PDFs, and text files
- **ğŸ¤– LangGraph Workflow**: Structured RAG pipeline using LangGraph state management
- **ğŸ’¾ Vector Storage**: FAISS-based vector store for efficient semantic search
- **ğŸ¯ Smart Chunking**: Recursive text splitting with configurable chunk sizes
- **ğŸ–¥ï¸ Interactive UI**: Clean Streamlit interface for document queries
- **âš¡ Fast Inference**: Powered by Groq's high-performance LLM API
- **ğŸ“Š Source Tracking**: View retrieved source documents for each answer

## ğŸ—ï¸ Architecture

The system follows a modular architecture with clear separation of concerns:

```
DocRag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/              # Configuration and API setup
â”‚   â”œâ”€â”€ document_ingestion/  # Document loading and processing
â”‚   â”œâ”€â”€ vectorstore/         # FAISS vector store management
â”‚   â”œâ”€â”€ graph_builder/       # LangGraph workflow construction
â”‚   â”œâ”€â”€ nodes/               # RAG workflow nodes
â”‚   â””â”€â”€ state/               # State management for LangGraph
â”œâ”€â”€ data/                    # Data storage directory
â”œâ”€â”€ streamlit_app.py         # Main Streamlit application
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ pyproject.toml          # Project metadata and dependencies
```

### Workflow

1. **Document Ingestion**: Load documents from URLs, PDFs, or text files
2. **Text Chunking**: Split documents into manageable chunks with overlap
3. **Embedding**: Convert text chunks into vector embeddings using HuggingFace models
4. **Vector Storage**: Store embeddings in FAISS for efficient retrieval
5. **Query Processing**: Retrieve relevant documents based on user queries
6. **Answer Generation**: Generate contextual answers using Groq LLM

## ğŸš€ Getting Started

### Prerequisites

- Python 3.13+
- Groq API key ([Get it here](https://console.groq.com/keys))
- HuggingFace API token (optional, for private models)

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/tatsat3mutee/DocRag.git
   cd DocRag
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

   Or using uv (recommended):

   ```bash
   uv pip install -r requirements.txt
   ```

3. **Set up environment variables**
   Create a `.env` file in the project root:

   ```env
   GROQ_API_KEY=your_groq_api_key_here
   HUGGINGFACE_API_KEY=your_huggingface_token_here  # Optional
   HUGGINGFACE_MODEL=sentence-transformers/all-MiniLM-L6-v2
   ```

### Running the Application

**Launch the Streamlit app:**

```bash
streamlit run streamlit_app.py
```

The application will be available at `http://localhost:8501`

## ğŸ“– Usage

### Using the Streamlit Interface

1. Launch the application
2. Wait for the system to initialize (loads default documents)
3. Enter your question in the search box
4. Click "ğŸ” Search" to get answers
5. Expand "ğŸ“„ Source Documents" to view retrieved context
6. View search history at the bottom of the page

### Default Documents

The system comes pre-configured with these documents:

- Lilian Weng's blog post on LLM Agents
- Lilian Weng's blog post on Diffusion Video Models

### Customizing Document Sources

Edit `src/config/config.py` to add your own URLs:

```python
DEFAULT_URLS = [
    "https://your-url-1.com",
    "https://your-url-2.com"
]
```

## ğŸ”§ Configuration

Key configuration options in `src/config/config.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `LLM_MODEL` | `qwen/qwen3-32b` | Groq model for answer generation |
| `EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | HuggingFace embedding model |
| `CHUNK_SIZE` | `500` | Text chunk size in characters |
| `CHUNK_OVERLAP` | `50` | Overlap between chunks |

## ğŸ› ï¸ Technology Stack

- **LangChain**: Framework for LLM applications
- **LangGraph**: Graph-based workflow orchestration
- **Groq**: High-performance LLM inference
- **FAISS**: Vector similarity search
- **HuggingFace**: Embedding models
- **Streamlit**: Web UI framework
- **BeautifulSoup**: Web scraping
- **PyPDF**: PDF document processing

## ğŸ“¦ Project Structure

### Core Components

- **DocumentProcessor**: Handles loading and chunking of documents from various sources
- **VectorStore**: Manages FAISS vector database and document retrieval
- **GraphBuilder**: Constructs LangGraph workflow with retrieval and generation nodes
- **RAGNodes**: Implements retrieval and answer generation logic
- **Config**: Centralized configuration management

### State Management

The system uses TypedDict-based state management for LangGraph:

```python
class RAGState(TypedDict):
    question: str              # User query
    retrieve_docs: List[Document]  # Retrieved documents
    answer: str               # Generated answer
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- [LangChain](https://www.langchain.com/) for the powerful LLM framework
- [Groq](https://groq.com/) for fast inference
- [HuggingFace](https://huggingface.co/) for embedding models
- Lilian Weng for the excellent blog posts used as default documents

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ using LangChain, LangGraph, and Streamlit**
